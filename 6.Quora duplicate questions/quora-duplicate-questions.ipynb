{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-06-23T22:54:34.100185Z","iopub.execute_input":"2022-06-23T22:54:34.100669Z","iopub.status.idle":"2022-06-23T22:54:34.108086Z","shell.execute_reply.started":"2022-06-23T22:54:34.100635Z","shell.execute_reply":"2022-06-23T22:54:34.105723Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Quora Duplicate qns\nCan you identify question pairs that have the same intent?\n\nAbout Dataset\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.","metadata":{}},{"cell_type":"code","source":"pip install contractions","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:54:34.122342Z","iopub.execute_input":"2022-06-23T22:54:34.123059Z","iopub.status.idle":"2022-06-23T22:54:42.503380Z","shell.execute_reply.started":"2022-06-23T22:54:34.123025Z","shell.execute_reply":"2022-06-23T22:54:42.501304Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:54:42.506320Z","iopub.execute_input":"2022-06-23T22:54:42.506806Z","iopub.status.idle":"2022-06-23T22:54:53.985933Z","shell.execute_reply.started":"2022-06-23T22:54:42.506771Z","shell.execute_reply":"2022-06-23T22:54:53.984795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:54:53.987263Z","iopub.execute_input":"2022-06-23T22:54:53.988155Z","iopub.status.idle":"2022-06-23T22:55:04.226603Z","shell.execute_reply.started":"2022-06-23T22:54:53.988124Z","shell.execute_reply":"2022-06-23T22:55:04.224748Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization, Dense, LSTM, Embedding, Lambda, InputLayer\nfrom tensorflow.keras import Sequential, Model, Input\n\nfrom collections import Counter, defaultdict\nimport contractions\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:11:04.331220Z","iopub.execute_input":"2022-06-23T23:11:04.332043Z","iopub.status.idle":"2022-06-23T23:11:04.342894Z","shell.execute_reply.started":"2022-06-23T23:11:04.332012Z","shell.execute_reply":"2022-06-23T23:11:04.341495Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"quora = pd.read_csv(\"/kaggle/input/quora-duplicate-qns/quora.csv\")\n# quora = pd.read_csv(\"quora.csv\")\nquora.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:04.247841Z","iopub.execute_input":"2022-06-23T22:55:04.248199Z","iopub.status.idle":"2022-06-23T22:55:05.131627Z","shell.execute_reply.started":"2022-06-23T22:55:04.248170Z","shell.execute_reply":"2022-06-23T22:55:05.130353Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"quora.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.133161Z","iopub.execute_input":"2022-06-23T22:55:05.133474Z","iopub.status.idle":"2022-06-23T22:55:05.143223Z","shell.execute_reply.started":"2022-06-23T22:55:05.133443Z","shell.execute_reply":"2022-06-23T22:55:05.141303Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"quora.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.145704Z","iopub.execute_input":"2022-06-23T22:55:05.146154Z","iopub.status.idle":"2022-06-23T22:55:05.183457Z","shell.execute_reply.started":"2022-06-23T22:55:05.146126Z","shell.execute_reply":"2022-06-23T22:55:05.182435Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for col in quora.select_dtypes(\"int64\").columns:\n    print(col, quora[col].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.185219Z","iopub.execute_input":"2022-06-23T22:55:05.185653Z","iopub.status.idle":"2022-06-23T22:55:05.213092Z","shell.execute_reply.started":"2022-06-23T22:55:05.185615Z","shell.execute_reply":"2022-06-23T22:55:05.212067Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(quora.qid1.value_counts()[:10])\nprint()\nprint(quora.qid2.value_counts()[:10])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.214324Z","iopub.execute_input":"2022-06-23T22:55:05.214749Z","iopub.status.idle":"2022-06-23T22:55:05.242028Z","shell.execute_reply.started":"2022-06-23T22:55:05.214707Z","shell.execute_reply":"2022-06-23T22:55:05.240040Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"quora.is_duplicate.value_counts(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.247941Z","iopub.execute_input":"2022-06-23T22:55:05.248355Z","iopub.status.idle":"2022-06-23T22:55:05.259722Z","shell.execute_reply.started":"2022-06-23T22:55:05.248314Z","shell.execute_reply":"2022-06-23T22:55:05.258465Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n## Split data","metadata":{}},{"cell_type":"code","source":"y = quora.is_duplicate\nX = quora[[\"qid1\", \"question1\", \"qid2\",\"question2\"]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 80,stratify = y)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size = 0.5, random_state = 11,stratify = y_test)\nprint(y_train.shape, y_val.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.261129Z","iopub.execute_input":"2022-06-23T22:55:05.261446Z","iopub.status.idle":"2022-06-23T22:55:05.371895Z","shell.execute_reply.started":"2022-06-23T22:55:05.261415Z","shell.execute_reply":"2022-06-23T22:55:05.370206Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.373201Z","iopub.execute_input":"2022-06-23T22:55:05.373507Z","iopub.status.idle":"2022-06-23T22:55:05.385285Z","shell.execute_reply.started":"2022-06-23T22:55:05.373481Z","shell.execute_reply":"2022-06-23T22:55:05.384153Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"X_val.drop(columns = [\"qid1\", \"qid2\"], inplace = True)\nX_test.drop(columns = [\"qid1\", \"qid2\"], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.387267Z","iopub.execute_input":"2022-06-23T22:55:05.387743Z","iopub.status.idle":"2022-06-23T22:55:05.407403Z","shell.execute_reply.started":"2022-06-23T22:55:05.387701Z","shell.execute_reply":"2022-06-23T22:55:05.405716Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.409526Z","iopub.execute_input":"2022-06-23T22:55:05.410707Z","iopub.status.idle":"2022-06-23T22:55:05.435198Z","shell.execute_reply.started":"2022-06-23T22:55:05.410659Z","shell.execute_reply":"2022-06-23T22:55:05.433209Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Duplicates in training set","metadata":{}},{"cell_type":"markdown","source":"As we see, the questions (qids) are not unique. For those duplicated question pairs, a question could be paired with several other questions, and they are all having similar meanings. We could group questions with similar meanings in a union, while questions with different meanings should be included in different unions.","metadata":{}},{"cell_type":"code","source":"def dfs(m: dict, visited: set, node: int) -> list:\n    res = [node]\n    for other in m[node]:\n        if other in visited:\n            continue\n        \n        visited.add(other)\n        res += dfs(m, visited, other)\n        \n    return res\n    \ndef find_qid_relation(X_train, y_train):\n    m = defaultdict(list)\n    \n    q1 = X_train.qid1.values\n    q2 = X_train.qid2.values\n    y_tmp = y_train.values\n    \n    for i in range(len(q1)):\n        if y_tmp[i]:\n            m[q1[i]].append(q2[i])\n            m[q2[i]].append(q1[i])\n      \n    num_union = 0\n    visited = set()  # store visited qids\n    qid_union = {}  # key: qid, value: the union this qid belongs to, duplicates belong to the same union\n    union_qid = []  # index: union, value: qids belongs to the same union\n    \n    for k in m.keys():\n        if k in visited:\n            continue\n            \n        visited.add(k)\n        nodes = dfs(m, visited, k)\n        for node in nodes:\n            qid_union[node] = num_union\n  \n        union_qid.append(nodes)\n        num_union += 1\n  \n    return union_qid, qid_union ","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.437156Z","iopub.execute_input":"2022-06-23T22:55:05.437593Z","iopub.status.idle":"2022-06-23T22:55:05.457745Z","shell.execute_reply.started":"2022-06-23T22:55:05.437529Z","shell.execute_reply":"2022-06-23T22:55:05.453108Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# the mapping between unions to qids, and qids to unions among duplicates\nunion_qid, qid_union = find_qid_relation(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.460636Z","iopub.execute_input":"2022-06-23T22:55:05.461014Z","iopub.status.idle":"2022-06-23T22:55:05.643033Z","shell.execute_reply.started":"2022-06-23T22:55:05.460987Z","shell.execute_reply":"2022-06-23T22:55:05.641048Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(r\"the number of unique qids corresponding to 'is_duplicate = 1':\", len(qid_union))\nprint(\"the number of unions:\", len(union_qid))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.645086Z","iopub.execute_input":"2022-06-23T22:55:05.645615Z","iopub.status.idle":"2022-06-23T22:55:05.654179Z","shell.execute_reply.started":"2022-06-23T22:55:05.645575Z","shell.execute_reply":"2022-06-23T22:55:05.652185Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Pre-process text","metadata":{}},{"cell_type":"code","source":"def fix_contractions(text):    # expand contraction\n    return contractions.fix(text)\n\ndef remove_punctuation(text):   # remove punctuations\n    return re.sub(r\"[^\\w\\d\\s]+\", '', text)\n\ndef tokenize(text):   # tokenize\n    return word_tokenize(text)\n\nstemmer = PorterStemmer()\ndef lemmatizer(tokens):    # stemming\n    return \" \".join([stemmer.stem(word, to_lowercase = True) for word in tokens ])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.655914Z","iopub.execute_input":"2022-06-23T22:55:05.656254Z","iopub.status.idle":"2022-06-23T22:55:05.670318Z","shell.execute_reply.started":"2022-06-23T22:55:05.656228Z","shell.execute_reply":"2022-06-23T22:55:05.669297Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"sample = \"What's going on at 1234 Carrier Dr? He can't concentrate on his assignment!!\"\nprint(\"original:\", sample)\nt2 = fix_contractions(sample)\nprint(\"\\nremove contraction:\",t2)\nt3 = remove_punctuation(t2)\nprint(\"\\nremove punctuation:\",t3)\nt4 = tokenize(t3)\nprint(\"\\ntokenize:\", t4)\nt5 = lemmatizer(t4)\nprint(\"\\nlemmatization:\", t5)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.672341Z","iopub.execute_input":"2022-06-23T22:55:05.672795Z","iopub.status.idle":"2022-06-23T22:55:05.704055Z","shell.execute_reply.started":"2022-06-23T22:55:05.672753Z","shell.execute_reply":"2022-06-23T22:55:05.702299Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def process_text(text):\n    text = fix_contractions(text)\n    text = remove_punctuation(text)\n    text = tokenize(text)\n    text = lemmatizer(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.705884Z","iopub.execute_input":"2022-06-23T22:55:05.706296Z","iopub.status.idle":"2022-06-23T22:55:05.716511Z","shell.execute_reply.started":"2022-06-23T22:55:05.706258Z","shell.execute_reply":"2022-06-23T22:55:05.715029Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def process_questions(X):\n    X.question1 = X.question1.apply(process_text)\n    X.question2 = X.question2.apply(process_text)\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.717410Z","iopub.execute_input":"2022-06-23T22:55:05.717703Z","iopub.status.idle":"2022-06-23T22:55:05.740441Z","shell.execute_reply.started":"2022-06-23T22:55:05.717677Z","shell.execute_reply":"2022-06-23T22:55:05.738788Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.743400Z","iopub.execute_input":"2022-06-23T22:55:05.744103Z","iopub.status.idle":"2022-06-23T22:55:05.764454Z","shell.execute_reply.started":"2022-06-23T22:55:05.744059Z","shell.execute_reply":"2022-06-23T22:55:05.763568Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"X_train = process_questions(X_train)\nX_val = process_questions(X_val)\nX_test = process_questions(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:55:05.765819Z","iopub.execute_input":"2022-06-23T22:55:05.767395Z","iopub.status.idle":"2022-06-23T22:56:24.424977Z","shell.execute_reply.started":"2022-06-23T22:55:05.767358Z","shell.execute_reply":"2022-06-23T22:56:24.422984Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:24.426428Z","iopub.execute_input":"2022-06-23T22:56:24.426718Z","iopub.status.idle":"2022-06-23T22:56:24.440989Z","shell.execute_reply.started":"2022-06-23T22:56:24.426691Z","shell.execute_reply":"2022-06-23T22:56:24.440038Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Vocabulary","metadata":{}},{"cell_type":"code","source":"q1_words = [word for question in X_train.question1.values for word in question.split(\" \")]\nq2_words = [word for question in X_train.question2.values for word in question.split(\" \")]\nunique_words = Counter(q1_words + q2_words) \nprint(\"the number of unique words in training set:\", len(unique_words))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:24.443293Z","iopub.execute_input":"2022-06-23T22:56:24.443786Z","iopub.status.idle":"2022-06-23T22:56:25.132393Z","shell.execute_reply.started":"2022-06-23T22:56:24.443758Z","shell.execute_reply":"2022-06-23T22:56:25.130889Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"first most frequent word:\", unique_words.most_common(1)[-1])\nprint(\"6000th most frequent word:\", unique_words.most_common(6000)[-1])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:25.133899Z","iopub.execute_input":"2022-06-23T22:56:25.134181Z","iopub.status.idle":"2022-06-23T22:56:25.212262Z","shell.execute_reply.started":"2022-06-23T22:56:25.134155Z","shell.execute_reply":"2022-06-23T22:56:25.211204Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"q1_sequence_len = [len(question.split(\" \")) for question in X_train.question1.values ]\nq2_sequence_len = [len(question.split(\" \")) for question in X_train.question2.values ]\npd.Series(q1_sequence_len + q2_sequence_len).hist();","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:25.213474Z","iopub.execute_input":"2022-06-23T22:56:25.213741Z","iopub.status.idle":"2022-06-23T22:56:25.534966Z","shell.execute_reply.started":"2022-06-23T22:56:25.213716Z","shell.execute_reply":"2022-06-23T22:56:25.533276Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(\"95% of the sequences with the length <=\", np.quantile(q1_sequence_len + q2_sequence_len, 0.95))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:25.543446Z","iopub.execute_input":"2022-06-23T22:56:25.543902Z","iopub.status.idle":"2022-06-23T22:56:25.578694Z","shell.execute_reply.started":"2022-06-23T22:56:25.543870Z","shell.execute_reply":"2022-06-23T22:56:25.577362Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"It could be a decent value to set maximum sequence length as 30.","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = 6000\nMAX_SEQUENCE_LEN = 30\n\n# A preprocessing layer which maps text features to integer sequences\ntext_vector = TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    standardize=None,\n    split='whitespace',\n    ngrams=None,\n    output_mode='int',\n    output_sequence_length=MAX_SEQUENCE_LEN,\n    pad_to_max_tokens=False)\n\ntext_vector.adapt(pd.concat([X_train.question1, X_train.question2], axis = 0))\ntext_vector.vocabulary_size()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:25.580603Z","iopub.execute_input":"2022-06-23T22:56:25.581171Z","iopub.status.idle":"2022-06-23T22:56:35.995493Z","shell.execute_reply.started":"2022-06-23T22:56:25.581130Z","shell.execute_reply":"2022-06-23T22:56:35.994339Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"X_train.question2.values[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:35.996511Z","iopub.execute_input":"2022-06-23T22:56:35.996757Z","iopub.status.idle":"2022-06-23T22:56:36.002696Z","shell.execute_reply.started":"2022-06-23T22:56:35.996729Z","shell.execute_reply":"2022-06-23T22:56:36.001877Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"text_vector(X_train.question2.values[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:36.003886Z","iopub.execute_input":"2022-06-23T22:56:36.004144Z","iopub.status.idle":"2022-06-23T22:56:36.032555Z","shell.execute_reply.started":"2022-06-23T22:56:36.004120Z","shell.execute_reply":"2022-06-23T22:56:36.030930Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def convert_text_to_sequence(X):\n    return text_vector(X.question1.values), text_vector(X.question2.values)\n    \nX_train_q1, X_train_q2 = convert_text_to_sequence(X_train)\nX_val_q1, X_val_q2 = convert_text_to_sequence(X_val)\nX_test_q1, X_test_q2 = convert_text_to_sequence(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:36.034615Z","iopub.execute_input":"2022-06-23T22:56:36.035186Z","iopub.status.idle":"2022-06-23T22:56:36.578031Z","shell.execute_reply.started":"2022-06-23T22:56:36.035141Z","shell.execute_reply":"2022-06-23T22:56:36.576857Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"X_train_q1[:3]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:36.580241Z","iopub.execute_input":"2022-06-23T22:56:36.581235Z","iopub.status.idle":"2022-06-23T22:56:36.590509Z","shell.execute_reply.started":"2022-06-23T22:56:36.581191Z","shell.execute_reply":"2022-06-23T22:56:36.589481Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Data generator","metadata":{}},{"cell_type":"markdown","source":"There're multiple questions occurring in the training set repeatedly. Let's map each unique qid to its question tensor, and store in a dictionary.","metadata":{}},{"cell_type":"code","source":"def qid_question(X, X_q1, X_q2):  # map qid to question tensor\n    m = {id: t for id, t in zip(X.qid1.values, X_q1)}\n    m.update({id: t for id, t in zip(X.qid2.values, X_q2)})\n    return m","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:36.591516Z","iopub.execute_input":"2022-06-23T22:56:36.592640Z","iopub.status.idle":"2022-06-23T22:56:36.611736Z","shell.execute_reply.started":"2022-06-23T22:56:36.592578Z","shell.execute_reply":"2022-06-23T22:56:36.609705Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"qid_tensor = qid_question(X_train, X_train_q1, X_train_q2)  # mapping between unique qids to the corresponding tensors in training set","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:56:36.613742Z","iopub.execute_input":"2022-06-23T22:56:36.614077Z","iopub.status.idle":"2022-06-23T22:57:08.840160Z","shell.execute_reply.started":"2022-06-23T22:56:36.614050Z","shell.execute_reply":"2022-06-23T22:57:08.838983Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"print(\"the number of unique questions (qids) in training set:\", len(qid_tensor))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:57:47.801050Z","iopub.execute_input":"2022-06-23T22:57:47.801451Z","iopub.status.idle":"2022-06-23T22:57:47.806185Z","shell.execute_reply.started":"2022-06-23T22:57:47.801414Z","shell.execute_reply":"2022-06-23T22:57:47.805577Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# def train_generator_v1(X, y, batch_size = 64):\n#     duplicates = X.loc[y == 1, [\"qid1\", \"qid2\"]]\n#     inputs = []\n    \n#     all_qids = list(set(np.concatenate((X.qid1.values, X.qid2.values))))  # all unique qids\n    \n#     while True:\n#         for i, row in duplicates.iterrows():\n#             qid1, qid2 = row[\"qid1\"], row[\"qid2\"]\n#             anc = qid_tensor[qid1]\n#             inputs.append([anc, qid_tensor[qid2]])\n\n#             neg_id = np.random.choice(all_qids)\n#             while neg_id == qid1 or neg_id == qid2:\n#                 neg_id = np.random.choice(all_qids)\n#             inputs.append([anc, qid_tensor[neg_id]])\n\n#             if len(inputs) == batch_size:\n#                 yield [np.array(inputs)], [1, 0] * (batch_size >> 1)\n\n#                 inputs = [] ","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:57:09.259575Z","iopub.status.idle":"2022-06-23T22:57:09.260298Z","shell.execute_reply.started":"2022-06-23T22:57:09.260055Z","shell.execute_reply":"2022-06-23T22:57:09.260079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_generator_v2(X, y, batch_size = 64):\n#     duplicates = X.loc[y == 1, [\"qid1\", \"qid2\"]]\n#     anchor = []\n#     other = []\n#     all_qids = list(set(np.concatenate((X.qid1.values, X.qid2.values))))  # all unique qids in X\n    \n#     while True:\n#         for i, row in duplicates.iterrows():\n#             qid1, qid2 = row[\"qid1\"], row[\"qid2\"]\n#             anc = qid_tensor[qid1]\n#             anchor += [anc, anc]\n\n#             neg_id = np.random.choice(all_qids)\n#             while neg_id == qid1 or neg_id == qid2:\n#                 neg_id = np.random.choice(all_qids)\n#             other += [qid_tensor[qid2], qid_tensor[neg_id]]\n\n#             if len(anchor) == batch_size:\n#                 yield [np.array(anchor), np.array(other)], np.array([1, 0] * (batch_size >> 1))\n\n#                 anchor = [] \n#                 other = []","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:57:09.261386Z","iopub.status.idle":"2022-06-23T22:57:09.262417Z","shell.execute_reply.started":"2022-06-23T22:57:09.262171Z","shell.execute_reply":"2022-06-23T22:57:09.262195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_qids = list(qid_tensor.keys())  # all unique qids in X_train\n\ndef train_generator_v3(batch_size = 64):\n    anchor = []\n    other = []\n    unions = [*range(len(union_qid))]\n    \n    while True:\n        for anchor_id, union in qid_union.items():\n            anc = qid_tensor[anchor_id]\n            anchor += [anc, anc]\n\n            pos_id = np.random.choice(union_qid[union])  # select a positive example from the same union\n            while pos_id == anchor_id:\n                pos_id = np.random.choice(union_qid[union])\n                \n            neg_id = np.random.choice(all_qids)  # select a negative example not from the same union\n            if neg_id in qid_union:\n                while qid_union[neg_id] == union:\n                    neg_id = np.random.choice(all_qids)\n                    \n                    if neg_id not in qid_union:\n                        break\n            \n            other += [qid_tensor[pos_id], qid_tensor[neg_id]]\n#             print(\"triplet:\",anchor_id, pos_id, neg_id)\n            if len(anchor) == batch_size:\n                yield [np.array(anchor), np.array(other)], np.array([1, 0] * (batch_size >> 1))\n\n                anchor = [] \n                other = []","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:57:59.878505Z","iopub.execute_input":"2022-06-23T22:57:59.878849Z","iopub.status.idle":"2022-06-23T22:57:59.892092Z","shell.execute_reply.started":"2022-06-23T22:57:59.878824Z","shell.execute_reply":"2022-06-23T22:57:59.891143Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"tmp = train_generator_v3(batch_size = 2)\nfor i in range(2):\n    print(next(tmp))\n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:58:01.480659Z","iopub.execute_input":"2022-06-23T22:58:01.481194Z","iopub.status.idle":"2022-06-23T22:58:01.527448Z","shell.execute_reply.started":"2022-06-23T22:58:01.481167Z","shell.execute_reply":"2022-06-23T22:58:01.525336Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"def history_plot(history):\n    plt.figure(figsize = (15,5));\n    \n    ax1 = plt.subplot(121);\n    ax1.plot(history[\"loss\"], color = 'r', marker = 'o', label = \"Train Loss\");\n    ax1.plot(history[\"val_loss\"], color = 'b',marker = 'o', label = \"Val Loss\");\n    plt.legend(loc=\"best\");\n    \n    ax2 = plt.subplot(122);    \n    ax2.plot(history[\"accuracy\"], color = 'r', marker = 'o', label = \"Train Accuracy\");\n    ax2.plot(history[\"val_accuracy\"], color = 'b', marker = 'o', label = \"Val Accuracy\");    \n    plt.legend(loc=\"best\");","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:58:06.944698Z","iopub.execute_input":"2022-06-23T22:58:06.945092Z","iopub.status.idle":"2022-06-23T22:58:06.954523Z","shell.execute_reply.started":"2022-06-23T22:58:06.945062Z","shell.execute_reply":"2022-06-23T22:58:06.953114Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"alpha = 0.6\n\ndef triplet_loss(v):\n    v1, v2 = v\n    v1 = v1.numpy()\n    v2 = v2.numpy()\n    score = np.dot(v1, v2.T)    \n    batch_size = score.shape[0]    \n    print(\"\\nv1:\\n\", v1, \"\\nv2:\\n\", v2, \"\\nscore:\\n\", score)\n    \n    pos = np.diag(score)\n    print(\"\\npositive:\\n\",pos)\n    \n    negatives = score - np.diag(pos)\n    print(\"\\nnegatives:\\n\",negatives)\n    \n    mean_negative = np.sum(negatives, axis = 1) / max((batch_size - 1),1)\n    print(\"\\nmean_negative:\\n\", mean_negative)\n    \n    closest_negative = np.max(negatives - 2 * np.eye(batch_size), axis = 1)\n    print(\"\\nclosest_neagtive:\\n\", closest_negative)    \n\n    trip_loss1 = np.maximum(-pos + mean_negative + alpha, 0)\n    trip_loss2 = np.maximum(-pos + closest_negative + alpha, 0)\n    loss = trip_loss1 + trip_loss2\n    print(\"\\ntrip_loss1:\\n\", trip_loss1,\"\\nloss:\\n\", loss)\n    \n    return tf.convert_to_tensor(loss.reshape(1, -1))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:58:08.950027Z","iopub.execute_input":"2022-06-23T22:58:08.950435Z","iopub.status.idle":"2022-06-23T22:58:08.964186Z","shell.execute_reply.started":"2022-06-23T22:58:08.950393Z","shell.execute_reply":"2022-06-23T22:58:08.962835Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"WORD_EMBEDDING_DIM = 30\nLEARNING_RATE = 0.0001\n\ndef normalized_by_column(X):\n    return X / tf.norm(X, axis = 1, keepdims=True)  # normalized by column\n\ndef base_net(X):\n    X = Embedding(input_dim = VOCAB_SIZE, output_dim = WORD_EMBEDDING_DIM, input_length = MAX_SEQUENCE_LEN)(X)\n    X = LSTM(16, dropout = 0.3)(X)\n    output = Lambda(function = normalized_by_column)(X)\n#     print(\"\\noutput:\\n\",output)\n    return output\n\n\ndef cosine_similarity(v):\n    v1, v2 = v\n    cos = tf.reduce_sum(v1 * v2, axis = -1,keepdims= True)\n    return tf.maximum(cos, 0)   # cosine similarity between 0 ~ 1\n\ndef Siamese():\n    Input1 = Input(shape=(MAX_SEQUENCE_LEN, ))   # used to instantiate a Keras tensor.\n    Input2 = Input(shape=(MAX_SEQUENCE_LEN, ))\n    \n    v1 = base_net(Input1)  # convert tensor to numpy array\n    v2 = base_net(Input2)\n\n    distance_layer = Lambda(function = cosine_similarity, name = \"similarity\")([v1, v2])\n    model = Model(inputs = [Input1, Input2], outputs = distance_layer)    \n    model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), metrics=[\"accuracy\"])\n    \n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:11:09.784579Z","iopub.execute_input":"2022-06-23T23:11:09.784973Z","iopub.status.idle":"2022-06-23T23:11:09.799614Z","shell.execute_reply.started":"2022-06-23T23:11:09.784940Z","shell.execute_reply":"2022-06-23T23:11:09.798571Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model = Siamese()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:11:11.845705Z","iopub.execute_input":"2022-06-23T23:11:11.846142Z","iopub.status.idle":"2022-06-23T23:11:12.476808Z","shell.execute_reply.started":"2022-06-23T23:11:11.846109Z","shell.execute_reply":"2022-06-23T23:11:12.475514Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# BATCH_SIZE = 64\n# train_set_size = 25000\n\n# history = model.fit(x = train_generator_v3(batch_size = BATCH_SIZE), \n#                     validation_data = ([X_val_q1[:1000], X_val_q2[:1000]], y_val[:1000]), \n#                     steps_per_epoch = train_set_size // BATCH_SIZE,\n#                     epochs = 20)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T22:57:09.273668Z","iopub.status.idle":"2022-06-23T22:57:09.274082Z","shell.execute_reply.started":"2022-06-23T22:57:09.273862Z","shell.execute_reply":"2022-06-23T22:57:09.273882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\ntrain_set_size = len(y_train) # 25000\n\nhistory = model.fit(x = [X_train_q1[:train_set_size], X_train_q2[:train_set_size]], \n                            y = y_train[:train_set_size],  \n                            batch_size = BATCH_SIZE, \n                            validation_data = ([X_val_q1, X_val_q2], y_val), \n                            validation_batch_size = BATCH_SIZE,\n                            epochs = 20)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:11:22.433852Z","iopub.execute_input":"2022-06-23T23:11:22.434199Z","iopub.status.idle":"2022-06-23T23:18:53.902749Z","shell.execute_reply.started":"2022-06-23T23:11:22.434173Z","shell.execute_reply":"2022-06-23T23:18:53.901122Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"history_plot(history.history)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:18:57.777473Z","iopub.execute_input":"2022-06-23T23:18:57.779036Z","iopub.status.idle":"2022-06-23T23:18:58.138969Z","shell.execute_reply.started":"2022-06-23T23:18:57.778983Z","shell.execute_reply":"2022-06-23T23:18:58.137712Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"print(\"Train:\")\nmodel.evaluate([X_train_q1, X_train_q2], y_train)\nprint(\"Validation:\")\nmodel.evaluate([X_val_q1, X_val_q2], y_val)\nprint(\"Test:\")\nmodel.evaluate([X_test_q1, X_test_q2], y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T23:19:12.928735Z","iopub.execute_input":"2022-06-23T23:19:12.929258Z","iopub.status.idle":"2022-06-23T23:19:32.740374Z","shell.execute_reply.started":"2022-06-23T23:19:12.929225Z","shell.execute_reply":"2022-06-23T23:19:32.739218Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}